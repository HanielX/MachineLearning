{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 这个算的还是有点问题啊\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_rec, ng_talk, ng_rec_target, ng_talk_target = load_rec_talk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3979 3253\n"
     ]
    }
   ],
   "source": [
    "print(len(ng_rec) , len(ng_talk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.concatenate 比 np.append 在拼接数组上更加高效?\n",
    "- np.append 只能进行两个数组的拼接，而np.concatenate 可以进行多个数组的拼接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证程序，至于为什么，有待验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3609109999999873\n"
     ]
    }
   ],
   "source": [
    "from time import clock as now\n",
    "a =  np.array(range(100000000))\n",
    "b =  np.array(range(100000000))\n",
    "time1 = now()\n",
    "all = np.append(a,b)\n",
    "time2 = now()\n",
    "print(time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0961079999999583\n"
     ]
    }
   ],
   "source": [
    "time1 = now()\n",
    "all = np.concatenate((a,b))\n",
    "time2 = now()\n",
    "print(time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: swdwan@napier.uwaterloo.ca (Donald Wan)\\nSubject: $ 80 SVX OIL CHANGE\\n \\nOrganization: University of Waterloo\\nLines: 17\\n\\n\\n\\n\\n\\n My friend brought a subaru SVX recently.  I had drove it for couples times and I\\nthink its a great car, esp on snow.  However when she took it to a local Subaru\\ndealer for a oil change, the bill came out to be about 80 dollars.  The dealer\\ntold us it is because to change the oil filter on a SVX it is necessary to\\ndisassemble a metal cover under the engine and that took an hour of labour.\\nAt first, we think we are being ripped off so she phone to a dealer in Toronto\\nbut found out the they are charging roughly the same price.  So is there any\\nSVX owner out there that has the same problem ?  And if the oil change story is\\ntrue, then the engineer of Subaru looks pretty stubid to me. By the way, the car\\nlooks great.\\n\\nSWD Wan.\\n\\n']"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_rec[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL 使用二元bigram 来计算一个句子出现的概率,句子概率计算的概率如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bigram](img/bigram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_rec_dict = defaultdict(int) # 一个元素 res\n",
    "two_rec_dict = defaultdict(int) # 两个元素\n",
    "one_talk_dict = defaultdict(int) # \n",
    "two_talk_dict = defaultdict(int) \n",
    "from nltk.corpus import stopwords\n",
    "# 使用停用词\n",
    "stopworddic = set(stopwords.words('english'))\n",
    "stopword = open('../../../tool/text/stopword_en.txt').read().split('\\n')\n",
    "for word in stopword:\n",
    "    stopworddic.add(word)\n",
    "# \n",
    "pattern = '[\\n\\t_*\\?\\\"-=\\[\\],<>:()\\.\\\\!/\\| ]+|[\\d]+' \n",
    "ng_rec_result = []\n",
    "ng_talk_result = []\n",
    "for ng_rec_i in ng_rec:\n",
    "    clean_data = re.split(pattern,ng_rec_i)\n",
    "    clean_data = [i for i in clean_data if i not in stopworddic ]\n",
    "    ng_rec_result.append(' '.join(clean_data))\n",
    "\n",
    "for ng_talk_i in ng_talk:\n",
    "    clean_data = re.split(pattern,ng_talk_i)\n",
    "    clean_data = [i for i in clean_data if i not in stopworddic]\n",
    "    ng_talk_result.append(' '.join(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3979"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ng_rec_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只取tfidf 值的前200个值\n",
    "ng_rec_final = []\n",
    "ng_talk_final = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "transform = vectorizer.fit_transform(ng_rec_result)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for trans in transform.toarray():\n",
    "    index = np.argsort(trans)[:]\n",
    "    ng_rec_final.append(np.array(feature_names)[index])\n",
    "transform = vectorizer.fit_transform(ng_talk_result)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for trans in transform.toarray():\n",
    "    index = np.argsort(trans)[:]\n",
    "    ng_talk_final.append(np.array(feature_names)[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ng_rec_i in ng_rec_final:\n",
    "    for i in range(len(ng_rec_i)):\n",
    "        if len(ng_rec_i[i]) == 1:\n",
    "            continue\n",
    "        one_rec_dict[ng_rec_i[i]] += 1\n",
    "for ng_talk_i in ng_talk_final:\n",
    "    for i in range(len(ng_talk_i)):\n",
    "        if len(ng_talk_i[i]) == 1:\n",
    "            continue\n",
    "        one_talk_dict[ng_talk_i[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计总的词数\n",
    "one_rec_count = 0\n",
    "one_talk_count = 0\n",
    "for val in one_rec_dict.values():\n",
    "    one_rec_count += val\n",
    "for val in one_talk_dict.values():\n",
    "    one_talk_count += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用2-gram的话，在一种出现的根本不会在二中出现\n",
    "rec_prob = []\n",
    "talk_prob = []\n",
    "for ng_rec_i in ng_rec_final:\n",
    "    p1 = 0; p2 = 0\n",
    "    flag = 0; \n",
    "    for i in range(len(ng_rec_i)):\n",
    "        if len(ng_rec_i[i]) == 1:\n",
    "            continue\n",
    "        p1 +=  np.log(one_rec_dict[ng_rec_i[i]] / one_rec_count)\n",
    "        if one_talk_dict[ng_rec_i[i]] == 0:\n",
    "            p2 = 0;\n",
    "            flag = 1\n",
    "        if flag == 0:\n",
    "            p2 += np.log(one_talk_dict[ng_rec_i[i]] / one_talk_count)\n",
    "    rec_prob.append(p1)\n",
    "    talk_prob.append(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_prob = np.array(talk_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res ^ talk 的KL 散度比较 \n",
    "talk_prob = talk_prob + 1e-12  # 出现零值时，对其的修正\n",
    "rec_prob /= np.sum(rec_prob)\n",
    "talk_prob /= np.sum(talk_prob)\n",
    "ss.entropy(rec_prob,talk_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "- 使用KL散度确实可以比较两个分布的相关性，数值越小，相关性越高，论文中得到的结果还没有我的好，不是不我做错了！\n",
    "- 两个文本类别之间的KL散度真的好难控制，因为只要有一个词不在对方的词库里面，那么其概率就是为0，就完全否定了相关性。\n",
    "- 使用1-gram 就已经很难去控制，更不要说使用2-gram了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
