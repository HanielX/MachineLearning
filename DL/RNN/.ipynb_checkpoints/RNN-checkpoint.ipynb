{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study website http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![种类](img/classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 像图片分类 一对一\n",
    "- 像图像中物体识别 一对多\n",
    "- 像情感分析 多对一\n",
    "- 像翻译 多对多\n",
    "- 实时翻译 多对多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![char](img/mini-char-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\begin{split}h_{t} &=tanh(W_{hx}x+W_{hh}h_{t-1}) \\\\ y &= softmax(W_{yh}h)\\end{split}$$\n",
    " - 定义好损失函数，就可以让下一个字符对应的位置尽可能地大，这样子在预测的时候就可以去向量中最大的值的索引做为待预测值的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉损失函数\n",
    "$$ J = - \\sum_{c=1}^{M}y_{c}log(p_{c})$$\n",
    "- M——类别的数量；\n",
    "- y——指示变量（0或1）,如果该类别和样本的类别相同就是1，否则是0；\n",
    "- p——对于观测样本属于类别c的预测概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DL 最全的优化方法](https://zhuanlan.zhihu.com/p/22252270)\n",
    "\n",
    "![adagrad](img/adagrad.jpg)\n",
    "- 采用累积平方梯度\n",
    "- 优点是使得更新的更加平缓，更新速度更快\n",
    "- 缺点是由于是累积平方梯度，导致学习率为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist_实验, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 定义一些参数\n",
    "import numpy as np\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "from time import time\n",
    "\n",
    "# unite_size = \n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mn = input_data.read_data_sets('MNIST_DATA',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterator = 20000;\n",
    "time_step = 28\n",
    "hidden_size = 400\n",
    "feature_size = 28\n",
    "import matplotlib.pyplot as plts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tanh(X):\n",
    "#     Y = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "#     return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tanh_overflow](img/tanh_overflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmod(X): ### nice\n",
    "#     return .5 * (1 + np.tanh(.5 * X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    sum_ = np.sum(np.exp(X), axis = 0)\n",
    "    y = np.exp(X) / sum_\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![adam](img/adam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这里参数矩阵的大小如何定义？每一个点为特征进行的加权\n",
    "### [hidden_size,feature_size]\n",
    "# Wx = np.random.standard_normal([hidden_size,feature_size])\n",
    "# ### [hidden_size,hidden_size]\n",
    "# Wh = np.random.standard_normal([hidden_size,hidden_size])\n",
    "# bh = np.zeros([hidden_size,1])\n",
    "# ### [class_size,hidden_size]\n",
    "# Wy = np.random.standard_normal([10,hidden_size])\n",
    "# by = np.zeros([10,1])\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "t = 0\n",
    "def init_params():\n",
    "    params = {}\n",
    "    params['h0'] = np.zeros([hidden_size,batch_size])\n",
    "    \n",
    "    params['Wx'] = np.random.standard_normal([hidden_size,feature_size])\n",
    "    params['Wh'] = np.random.standard_normal([hidden_size,hidden_size])\n",
    "    \n",
    "    params['bh'] = np.random.standard_normal([hidden_size,1])\n",
    "    \n",
    "    params['mWx_m'] = np.zeros_like(params['Wx'])\n",
    "    params['mWh_m'] = np.zeros_like(params['Wh'])\n",
    "    params['mbh_m'] = np.zeros_like(params['bh'])\n",
    "    \n",
    "    params['mWx_v'] = np.zeros_like(params['Wx'])\n",
    "    params['mWh_v'] = np.zeros_like(params['Wh'])\n",
    "    params['mbh_v'] = np.zeros_like(params['bh'])\n",
    "    \n",
    "    params['Wy'] = np.random.standard_normal([10,hidden_size])\n",
    "    \n",
    "    params['by'] = np.random.standard_normal([10,1])\n",
    "    \n",
    "    params['mWy_m'] = np.zeros_like(params['Wy'])\n",
    "    params['mWy_v'] = np.zeros_like(params['Wy'])\n",
    "    \n",
    "    params['mby_m'] = np.zeros_like(params['by'])\n",
    "    params['mby_v'] = np.zeros_like(params['by'])\n",
    "    return params\n",
    "\n",
    "def forward(X,label,params):\n",
    "    global t,beta1,beta2\n",
    "#     print(beta1, beta2)\n",
    "    t += 1\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    \n",
    "    for i in range(time_step): ## 每一步拿出所有数据的第一行来进行训练\n",
    "        x = X[:,i,:]\n",
    "        params['h' + str(i + 1)] = np.tanh(np.dot(params['Wh'], params['h'+ str(i)]) + params['bh'] + np.dot(params['Wx'],x.transpose()))\n",
    "        \n",
    "    y = softmax(np.dot(params['Wy'] ,params['h' + str(time_step)]) + params['by'])\n",
    "    \n",
    "    y_pred = y\n",
    "\n",
    "    index = label.argmax(axis = 1)\n",
    "    y_pred[index,range(y_pred.shape[1])] -= 1\n",
    "    y_pred /= batch_size \n",
    "    \n",
    "    dwy = np.dot(y_pred, params['h' + str(time_step)].transpose())\n",
    "    dby = np.sum(y_pred, axis = 1,keepdims = True)\n",
    "    dht = np.dot(params['Wy'].transpose(), y_pred)\n",
    "    \n",
    "#     params['mWy'] += dwy * dwy\n",
    "#     params['mby'] += dby * dby\n",
    "    params['mWy_m'] = beta1 *  params['mWy_m'] + (1 - beta1) * dwy\n",
    "    params['mWy_v'] = beta2 * params['mWy_v'] + (1 - beta2) * dwy * dwy\n",
    "    mWy_m_tmp = params['mWy_m'] / (1 - np.power(beta1, t))\n",
    "    mWy_v_tmp =  params['mWy_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['mby_m'] = beta1 *  params['mby_m'] + (1 - beta1) * dby\n",
    "    params['mby_v'] = beta2 * params['mby_v'] + (1 - beta2) * dby * dby\n",
    "    mby_m_tmp = params['mby_m'] / (1 - np.power(beta1, t))\n",
    "    mby_v_tmp =  params['mby_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['Wy'] += -learning_rate * mWy_m_tmp / (np.sqrt(mWy_v_tmp) + 1e-8)\n",
    "    params['by'] += -learning_rate * mby_m_tmp / (np.sqrt(mby_v_tmp) + 1e-8)\n",
    "    \n",
    "    dwh_tmp = np.zeros_like(params['Wh'])\n",
    "    dbh_tmp = np.zeros_like(params['bh'])\n",
    "    dwx_tmp = np.zeros_like(params['Wx'])\n",
    "    \n",
    "    \n",
    "#     X = X.reshape(-1,time_step,feature_size)\n",
    "    ## 会出现梯度消失的问题\n",
    "    \n",
    "    ## 将对应的偏 ht/ 偏ht-1 等于1\n",
    "    \n",
    "    for i in reversed(range(time_step)):\n",
    "        x = X[:,i,:]\n",
    "        dt = (1 - (params['h' + str(i + 1)] ** 2)) * dht\n",
    "        dt_t_1_h = np.dot(dt , params['h' + str(i)].transpose()) ## be careful \n",
    "        dt_t_1_x = np.dot(dt , x)\n",
    "        dt_t_1_b = np.sum(dt * dht, axis = 1, keepdims = True)\n",
    "\n",
    "        dwh_tmp += dt_t_1_h\n",
    "        dbh_tmp += dt_t_1_b\n",
    "        dwx_tmp += dt_t_1_x\n",
    "    ### 使用等于0的算法\n",
    "    \n",
    "#     dt = (1 - (params['h' + str(time_step)] ** 2)) * dht\n",
    "#     dt_t_1_h = np.dot(dt , params['h' + str(i)].transpose()) ## be careful \n",
    "#     dt_t_1_x = np.dot(dt , x)\n",
    "#     dt_t_1_b = np.sum(dt * dht, axis = 1, keepdims = True)\n",
    "\n",
    "    dt_t_1_h = dwh_tmp\n",
    "    dt_t_1_b = dbh_tmp\n",
    "    dt_t_1_x = dwx_tmp\n",
    "    \n",
    "    params['mWx_m'] = beta1 *  params['mWx_m'] + (1 - beta1) * dt_t_1_x\n",
    "    params['mWx_v'] = beta2 * params['mWx_v'] + (1 - beta2) * dt_t_1_x * dt_t_1_x\n",
    "    mWx_m_tmp = params['mWx_m'] / (1 - np.power(beta1, t))\n",
    "    mWx_v_tmp =  params['mWx_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['mWh_m'] = beta1 *  params['mWh_m'] + (1 - beta1) * dt_t_1_h\n",
    "    params['mWh_v'] = beta2 * params['mWh_v'] + (1 - beta2) * dt_t_1_h * dt_t_1_h\n",
    "    mWh_m_tmp = params['mWh_m'] / (1 - np.power(beta1, t))\n",
    "    mWh_v_tmp =  params['mWh_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['mbh_m'] = beta1 *  params['mbh_m'] + (1 - beta1) * dt_t_1_b\n",
    "    params['mbh_v'] = beta2 * params['mbh_v'] + (1 - beta2) * dt_t_1_b * dt_t_1_b\n",
    "    mbh_m_tmp = params['mbh_m'] / (1 - np.power(beta1, t))\n",
    "    mbh_v_tmp =  params['mbh_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['Wx'] = -learning_rate * mWx_m_tmp / (np.sqrt(mWx_v_tmp) + 1e-8)\n",
    "    params['Wh'] = -learning_rate * mWh_m_tmp / (np.sqrt(mWh_v_tmp) + 1e-8)\n",
    "    params['bh'] = -learning_rate * mbh_m_tmp / (np.sqrt(mbh_v_tmp) + 1e-8)\n",
    "\n",
    "# #  adagrad 梯度\n",
    "#     params['mWx'] += dwx_tmp * dt_t_1_x\n",
    "#     params['mWh'] += dwh_tmp * dt_t_1_h\n",
    "#     params['mbh'] += dbh_tmp * dt_t_1_b\n",
    "#     params['Wx'] = -learning_rate * dwx_tmp / np.sqrt(params['mWx']  + 1e-8)\n",
    "#     params['Wh'] = -learning_rate * dwh_tmp / np.sqrt(params['mWh']  + 1e-8)\n",
    "#     params['bh'] = -learning_rate * dbh_tmp / np.sqrt(params['mbh']  + 1e-8)\n",
    "\n",
    "    return params\n",
    "def predict(X, params):\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    for i in range(time_step): ## 每一步拿出所有数据的第一行来进行训练\n",
    "        x = X[:,i,:]\n",
    "        params['h' + str(i + 1)] = np.tanh(np.dot(params['Wh'], params['h'+ str(i)]) + params['bh']  + np.dot(params['Wx'],x.transpose()))\n",
    "    y = softmax(np.dot(params['Wy'],params['h' + str(time_step)]) + params['by'])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "params = init_params()\n",
    "start = time()\n",
    "for iterator in range(num_iterator):\n",
    "    train_data,train_label = mn.train.next_batch(batch_size) ### random\n",
    "    params = forward(train_data,train_label,params)\n",
    "    pred_y = predict(train_data,params)\n",
    "    \n",
    "    result = np.dot(train_label,pred_y)\n",
    "    result[result < 1e-8] = 1e-8\n",
    "    result[result >= 1] = 0.99999\n",
    "    loss = - np.trace(np.log(result))\n",
    "    # print(loss)\n",
    "    loss_list.append(loss)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_y = predict(train_data,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227.923778637185"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = train_label.argmax(axis = 1)\n",
    "# -np.sum(np.log(pred_y[index,range(pred_y.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SGD 会loss 函数会上下抖动。\n",
    "## 确实可以收敛，但是不是收敛到最小，所以还是有点问题\n",
    "## 只是说这里逐渐趋于稳定，但是不逐渐减小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss_list)), loss_list)\n",
    "plt.savefig('rnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2285.793159361234"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y = mn.test.next_batch(batch_size)\n",
    "y = predict(test_x,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.123"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(y, axis = 0) == np.argmax(test_y,axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 该$RNN$中涉及的求导公式主要有\n",
    "    - $tanh(x) = 1 - tanh^{2}(x)$\n",
    "    - $\\frac{\\partial WX}{\\partial W} = X^{T}, \\frac{\\partial WX}{\\partial X} = W^{T}$\n",
    "    - softmax 的求导。$S_{j} = \\frac{e_{j}}{\\sum_{i=0}^{n} e_{i}}$\n",
    "        - 当 $i!=j$时，$\\frac{\\partial S_{j}}{\\partial i} = -S_{i}S_{j}$\n",
    "        - 当 $i == j$时，$\\frac{\\partial S_{i}}{\\partial i} = S_{i} \\times (1 - S_{i})$\n",
    "- 果然自己手写RNN,还是会有很多的问题的，比如梯度消失，爆炸，函数的定义，我竟然想着在每一个时刻都用一个$Wx$和$Wh$来作为参数，这真的是愚蠢之极。\n",
    "- 关于梯度消失的问题是因为你逐渐求导的话，会出现$\\prod_{k = 0}^{t} H'(t) Wh$,因为$H'(t)$是小于等于1的，而$H'(t)$中的数值一般也是小于零的，所以随着时间的推移，该值会逐渐减小，所以会出现梯度消失的情况。\n",
    "- 虽然收敛了，但是为什么就只收敛了一部分？这还是一个问题！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 速度实验验证\n",
    "- $$\\begin{array}{c|ccc} \n",
    "结果 & 本机 & Kaggle & google \\\\\n",
    "\\hline\n",
    "CPU & 110.7 & 305.9 & 415.2\n",
    "\\end{array}$$\n",
    "\n",
    "- $$\\begin{array}{c|ccc} \n",
    "结果 & 本机(CPU) & Kaggle & google \\\\\n",
    "\\hline\n",
    "GPU &  91.7 & 38.7 & 66.93\n",
    "\\end{array}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
