{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study website http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![种类](img/classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 像图片分类 一对一\n",
    "- 像图像中物体识别 一对多\n",
    "- 像情感分析 多对一\n",
    "- 像翻译 多对多\n",
    "- 实时翻译 多对多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![char](img/mini-char-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\begin{split}h_{t} &=tanh(W_{hx}x+W_{hh}h_{t-1}) \\\\ y &= softmax(W_{yh}h)\\end{split}$$\n",
    " - 定义好损失函数，就可以让下一个字符对应的位置尽可能地大，这样子在预测的时候就可以去向量中最大的值的索引做为待预测值的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉损失函数\n",
    "$$ J = - \\sum_{c=1}^{M}y_{c}log(p_{c})$$\n",
    "- M——类别的数量；\n",
    "- y——指示变量（0或1）,如果该类别和样本的类别相同就是1，否则是0；\n",
    "- p——对于观测样本属于类别c的预测概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DL 最全的优化方法](https://zhuanlan.zhihu.com/p/22252270)\n",
    "\n",
    "![adagrad](img/adagrad.jpg)\n",
    "- 采用累积平方梯度\n",
    "- 优点是使得更新的更加平缓，更新速度更快\n",
    "- 缺点是由于是累积平方梯度，导致学习率为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist_实验, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 定义一些参数\n",
    "import numpy as np\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "from time import time\n",
    "from numpy.linalg import *\n",
    "# unite_size = \n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mn = input_data.read_data_sets('MNIST_DATA',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterator = 20000;\n",
    "time_step = 28\n",
    "hidden_size = 400\n",
    "feature_size = 28\n",
    "import matplotlib.pyplot as plts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tanh(X):\n",
    "#     Y = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "#     return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tanh_overflow](img/tanh_overflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmod(X): ### nice\n",
    "#     return .5 * (1 + np.tanh(.5 * X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    sum_ = np.sum(np.exp(X), axis = 0)\n",
    "    y = np.exp(X) / sum_\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![adam](img/adam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这里参数矩阵的大小如何定义？每一个点为特征进行的加权\n",
    "### [hidden_size,feature_size]\n",
    "# Wx = np.random.standard_normal([hidden_size,feature_size])\n",
    "# ### [hidden_size,hidden_size]\n",
    "# Wh = np.random.standard_normal([hidden_size,hidden_size])\n",
    "# bh = np.zeros([hidden_size,1])\n",
    "# ### [class_size,hidden_size]\n",
    "# Wy = np.random.standard_normal([10,hidden_size])\n",
    "# by = np.zeros([10,1])\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "t = 0\n",
    "decay_step = 1000\n",
    "decay_rate = 0.96\n",
    "def init_params():\n",
    "    params = {}\n",
    "    params['h0'] = np.zeros([hidden_size,batch_size])\n",
    "    \n",
    "    params['Wx'] = np.random.standard_normal([hidden_size,feature_size])\n",
    "    params['Wh'] = np.random.standard_normal([hidden_size,hidden_size])\n",
    "    \n",
    "    params['bh'] = np.random.standard_normal([hidden_size,1])\n",
    "    \n",
    "    params['mWx_m'] = np.zeros_like(params['Wx'])\n",
    "    params['mWh_m'] = np.zeros_like(params['Wh'])\n",
    "    params['mbh_m'] = np.zeros_like(params['bh'])\n",
    "    \n",
    "    params['mWx_v'] = np.zeros_like(params['Wx'])\n",
    "    params['mWh_v'] = np.zeros_like(params['Wh'])\n",
    "    params['mbh_v'] = np.zeros_like(params['bh'])\n",
    "    \n",
    "    params['Wy'] = np.random.standard_normal([10,hidden_size])\n",
    "    \n",
    "    params['by'] = np.random.standard_normal([10,1])\n",
    "    \n",
    "    params['mWy_m'] = np.zeros_like(params['Wy'])\n",
    "    params['mWy_v'] = np.zeros_like(params['Wy'])\n",
    "    \n",
    "    params['mby_m'] = np.zeros_like(params['by'])\n",
    "    params['mby_v'] = np.zeros_like(params['by'])\n",
    "    return params\n",
    "\n",
    "def forward(X,label,params):\n",
    "    global t,beta1,beta2\n",
    "#     print(beta1, beta2)\n",
    "    t += 1\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    \n",
    "    for i in range(time_step): ## 每一步拿出所有数据的第一行来进行训练\n",
    "        x = X[:,i,:]\n",
    "        params['h' + str(i + 1)] = np.tanh(np.dot(params['Wh'], params['h'+ str(i)]) + params['bh'] + np.dot(params['Wx'],x.transpose()))\n",
    "        \n",
    "    y = softmax(np.dot(params['Wy'] ,params['h' + str(time_step)]) + params['by'])\n",
    "    \n",
    "    y_pred = y\n",
    "\n",
    "    index = label.argmax(axis = 1)\n",
    "    y_pred[index,range(y_pred.shape[1])] -= 1\n",
    "    y_pred /= batch_size \n",
    "    \n",
    "    dwy = np.dot(y_pred, params['h' + str(time_step)].transpose())\n",
    "    dby = np.sum(y_pred, axis = 1,keepdims = True)\n",
    "    dht = np.dot(params['Wy'].transpose(), y_pred)\n",
    "    \n",
    "#     params['mWy'] += dwy * dwy\n",
    "#     params['mby'] += dby * dby\n",
    "    params['mWy_m'] = beta1 *  params['mWy_m'] + (1 - beta1) * dwy\n",
    "    params['mWy_v'] = beta2 * params['mWy_v'] + (1 - beta2) * dwy * dwy\n",
    "    mWy_m_tmp = params['mWy_m'] / (1 - np.power(beta1, t))\n",
    "    mWy_v_tmp =  params['mWy_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['mby_m'] = beta1 *  params['mby_m'] + (1 - beta1) * dby\n",
    "    params['mby_v'] = beta2 * params['mby_v'] + (1 - beta2) * dby * dby\n",
    "    mby_m_tmp = params['mby_m'] / (1 - np.power(beta1, t))\n",
    "    mby_v_tmp =  params['mby_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['Wy'] += -learning_rate * np.power(decay_rate, t / decay_step) * mWy_m_tmp / (np.sqrt(mWy_v_tmp) + 1e-8)\n",
    "    params['by'] += -learning_rate * np.power(decay_rate, t / decay_step) * mby_m_tmp / (np.sqrt(mby_v_tmp) + 1e-8)\n",
    "    \n",
    "    dwh_tmp = np.zeros_like(params['Wh'])\n",
    "    dbh_tmp = np.zeros_like(params['bh'])\n",
    "    dwx_tmp = np.zeros_like(params['Wx'])\n",
    "    \n",
    "    \n",
    "#     X = X.reshape(-1,time_step,feature_size)\n",
    "    ## 会出现梯度消失的问题\n",
    "    \n",
    "    ## 将对应的偏 ht/ 偏ht-1 等于1\n",
    "    \n",
    "    for i in reversed(range(time_step)):\n",
    "        x = X[:,i,:]\n",
    "        dt = np.dot(np.diag(1 - (params['h' + str(i + 1)] ** 2)), dht)\n",
    "        dt_t_1_h = np.dot(dt , params['h' + str(i)].transpose()) ## be careful \n",
    "        dt_t_1_x = np.dot(dt , x)\n",
    "        dt_t_1_b = np.sum(dt * dht, axis = 1, keepdims = True)\n",
    "\n",
    "        dwh_tmp += dt_t_1_h\n",
    "        dbh_tmp += dt_t_1_b\n",
    "        dwx_tmp += dt_t_1_x\n",
    "    ### 使用等于0的算法\n",
    "    \n",
    "#     dt = (1 - (params['h' + str(time_step)] ** 2)) * dht\n",
    "#     dt_t_1_h = np.dot(dt , params['h' + str(i)].transpose()) ## be careful \n",
    "#     dt_t_1_x = np.dot(dt , x)\n",
    "#     dt_t_1_b = np.sum(dt * dht, axis = 1, keepdims = True)\n",
    "\n",
    "    dt_t_1_h = dwh_tmp\n",
    "    dt_t_1_b = dbh_tmp\n",
    "    dt_t_1_x = dwx_tmp\n",
    "    \n",
    "    params['mWx_m'] = beta1 *  params['mWx_m'] + (1 - beta1) * dt_t_1_x\n",
    "    params['mWx_v'] = beta2 * params['mWx_v'] + (1 - beta2) * dt_t_1_x * dt_t_1_x\n",
    "    mWx_m_tmp = params['mWx_m'] / (1 - np.power(beta1, t))\n",
    "    mWx_v_tmp =  params['mWx_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['mWh_m'] = beta1 *  params['mWh_m'] + (1 - beta1) * dt_t_1_h\n",
    "    params['mWh_v'] = beta2 * params['mWh_v'] + (1 - beta2) * dt_t_1_h * dt_t_1_h\n",
    "    mWh_m_tmp = params['mWh_m'] / (1 - np.power(beta1, t))\n",
    "    mWh_v_tmp =  params['mWh_v'] / (1 - np.power(beta2, t))\n",
    "    \n",
    "    params['mbh_m'] = beta1 *  params['mbh_m'] + (1 - beta1) * dt_t_1_b\n",
    "    params['mbh_v'] = beta2 * params['mbh_v'] + (1 - beta2) * dt_t_1_b * dt_t_1_b\n",
    "    mbh_m_tmp = params['mbh_m'] / (1 - np.power(beta1, t))\n",
    "    mbh_v_tmp =  params['mbh_v'] / (1 - np.power(beta2, t))\n",
    "#     print('梯度:',norm(mWx_m_tmp / np.sqrt(mWx_v_tmp)), norm(mWh_m_tmp / np.sqrt(mWh_v_tmp))) \n",
    "    wx_cut = mWx_m_tmp / (np.sqrt(mWx_v_tmp) + 1e-8)\n",
    "    wh_cut = mWh_m_tmp / (np.sqrt(mWh_v_tmp) + 1e-8)\n",
    "    bh_cut = mbh_m_tmp / (np.sqrt(mbh_v_tmp) + 1e-8)\n",
    "    if norm(wx_cut) > 2:\n",
    "        wx_cut = 2 * wx_cut / norm(wx_cut)\n",
    "    if norm(wh_cut) > 2:\n",
    "        wh_cut = 2 * wh_cut / norm(wh_cut)\n",
    "    if norm(bh_cut) > 2:\n",
    "        bh_cut = 2 * bh_cut / norm(bh_cut)\n",
    "    \n",
    "    params['Wx'] = -learning_rate  * np.power(decay_rate, t / decay_step) * wx_cut\n",
    "    params['Wh'] = -learning_rate * np.power(decay_rate, t / decay_step) * wh_cut\n",
    "    params['bh'] = -learning_rate * np.power(decay_rate, t / decay_step) * bh_cut\n",
    "\n",
    "# #  adagrad 梯度\n",
    "#     params['mWx'] += dwx_tmp * dt_t_1_x\n",
    "#     params['mWh'] += dwh_tmp * dt_t_1_h\n",
    "#     params['mbh'] += dbh_tmp * dt_t_1_b\n",
    "#     params['Wx'] = -learning_rate * dwx_tmp / np.sqrt(params['mWx']  + 1e-8)\n",
    "#     params['Wh'] = -learning_rate * dwh_tmp / np.sqrt(params['mWh']  + 1e-8)\n",
    "#     params['bh'] = -learning_rate * dbh_tmp / np.sqrt(params['mbh']  + 1e-8)\n",
    "\n",
    "    return params\n",
    "def predict(X, params):\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    for i in range(time_step): ## 每一步拿出所有数据的第一行来进行训练\n",
    "        x = X[:,i,:]\n",
    "        params['h' + str(i + 1)] = np.tanh(np.dot(params['Wh'], params['h'+ str(i)]) + params['bh']  + np.dot(params['Wx'],x.transpose()))\n",
    "    y = softmax(np.dot(params['Wy'],params['h' + str(time_step)]) + params['by'])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "params = init_params()\n",
    "start = time()\n",
    "train_data,train_label = mn.train.next_batch(batch_size) ### random\n",
    "for iterator in range(num_iterator):\n",
    "\n",
    "    params = forward(train_data,train_label,params)\n",
    "    pred_y = predict(train_data,params)\n",
    "    \n",
    "    result = np.dot(train_label,pred_y)\n",
    "    result[result < 1e-8] = 1e-8\n",
    "    result[result >= 1] = 0.99999\n",
    "    loss = - np.trace(np.log(result))\n",
    "#     if iterator % 20 == 0:\n",
    "#         print(loss/batch_size)\n",
    "    loss_list.append(loss/batch_size)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_y = predict(train_data,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227.923778637185"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = train_label.argmax(axis = 1)\n",
    "# -np.sum(np.log(pred_y[index,range(pred_y.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SGD 会loss 函数会上下抖动。\n",
    "## 确实可以收敛，但是不是收敛到最小，所以还是有点问题\n",
    "## 只是说这里逐渐趋于稳定，但是不逐渐减小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss_list)), loss_list)\n",
    "plt.savefig('rnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2285.793159361234"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y = mn.test.next_batch(batch_size)\n",
    "y = predict(test_x,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.123"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(y, axis = 0) == np.argmax(test_y,axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 该$RNN$中涉及的求导公式主要有\n",
    "    - $tanh(x) = 1 - tanh^{2}(x)$\n",
    "    - $\\frac{\\partial WX}{\\partial W} = X^{T}, \\frac{\\partial WX}{\\partial X} = W^{T}$\n",
    "    - softmax 的求导。$S_{j} = \\frac{e_{j}}{\\sum_{i=0}^{n} e_{i}}$\n",
    "        - 当 $i!=j$时，$\\frac{\\partial S_{j}}{\\partial i} = -S_{i}S_{j}$\n",
    "        - 当 $i == j$时，$\\frac{\\partial S_{i}}{\\partial i} = S_{i} \\times (1 - S_{i})$\n",
    "- 果然自己手写RNN,还是会有很多的问题的，比如梯度消失，爆炸，函数的定义，我竟然想着在每一个时刻都用一个$Wx$和$Wh$来作为参数，这真的是愚蠢之极。\n",
    "- 关于梯度消失的问题是因为你逐渐求导的话，会出现$\\prod_{k = 0}^{t} H'(t) Wh$,因为$H'(t)$是小于等于1的，而$H'(t)$中的数值一般也是小于零的，所以随着时间的推移，该值会逐渐减小，所以会出现梯度消失的情况。\n",
    "- 虽然收敛了，但是为什么就只收敛了一部分？这还是一个问题！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 速度实验验证\n",
    "- $$\\begin{array}{c|ccc} \n",
    "result(s) & local & Kaggle & google \\\\\n",
    "\\hline\n",
    "CPU & 110.7 & 305.9 & 415.2\n",
    "\\end{array}$$\n",
    "\n",
    "- $$\\begin{array}{c|ccc} \n",
    "result(s) & local(CPU) & Kaggle & google \\\\\n",
    "\\hline\n",
    "GPU &  91.7 & 38.7 & 66.93\n",
    "\\end{array}$$\n",
    "- 在相同的数据下进行的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array([1,2,3])\n",
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  9],\n",
       "       [ 4, 10, 18],\n",
       "       [ 7, 16, 27]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 8, 10, 12],\n",
       "       [21, 24, 27]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 0, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c8e3f02cf9a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'norm'"
     ]
    }
   ],
   "source": [
    "np.norm([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
