{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study website http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![种类](img/classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 像图片分类 一对一\n",
    "- 像图像中物体识别 一对多\n",
    "- 像情感分析 多对一\n",
    "- 像翻译 多对多\n",
    "- 实时翻译 多对多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![char](img/mini-char-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\begin{split}h_{t} &=tanh(W_{hx}x+W_{hh}h_{t-1}) \\\\ y &= softmax(W_{yh}h)\\end{split}$$\n",
    " - 定义好损失函数，就可以让下一个字符对应的位置尽可能地大，这样子在预测的时候就可以去向量中最大的值的索引做为待预测值的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉损失函数\n",
    "$$ J = - \\sum_{c=1}^{M}y_{c}log(p_{c})$$\n",
    "- M——类别的数量；\n",
    "- y——指示变量（0或1）,如果该类别和样本的类别相同就是1，否则是0；\n",
    "- p——对于观测样本属于类别c的预测概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DL 最全的优化方法](https://zhuanlan.zhihu.com/p/22252270)\n",
    "\n",
    "![adagrad](img/adagrad.jpg)\n",
    "- 采用累积平方梯度\n",
    "- 优点是使得更新的更加平缓，更新速度更快\n",
    "- 缺点是由于是累积平方梯度，导致学习率为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist_实验, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 定义一些参数\n",
    "import numpy as np\n",
    "learning_rate = 1e-4\n",
    "batch_size = 10000\n",
    "# unite_size = \n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mn = input_data.read_data_sets('MNIST_DATA',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterator = 100;\n",
    "time_step = 28\n",
    "hidden_size = 200\n",
    "feature_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tanh(X):\n",
    "#     Y = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "#     return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X): ### nice\n",
    "    return .5 * (1 + np.tanh(.5 * X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这里参数矩阵的大小如何定义？每一个点为特征进行的加权\n",
    "### [hidden_size,feature_size]\n",
    "Wx = np.random.standard_normal([hidden_size,feature_size])\n",
    "bx = np.zeros([hidden_size,1])\n",
    "### [hidden_size,hidden_size]\n",
    "Wh = np.random.standard_normal([hidden_size,hidden_size])\n",
    "bh = np.zeros([hidden_size,1])\n",
    "### [class_size,hidden_size]\n",
    "Wy = np.random.standard_normal([10,hidden_size])\n",
    "by = np.zeros([10,1])\n",
    "def forward(X,label):\n",
    "    global Wx,bx,Wh,bh,Wy,by\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    params['h0'] = np.zeros([hidden_size,batch_size])\n",
    "    for i in range(time_step): ## 每一步拿出所有数据的第一行来进行训练\n",
    "        x = X[:,i,:]\n",
    "        params['h' + str(i + 1)] = np.tanh(np.dot(Wh, params['h'+ str(i)]) + bh + np.dot(Wx,x.transpose()) + bx)\n",
    "    y = softmax(np.dot(Wy,params['h' + str(time_step)]) + by)\n",
    "    \n",
    "    y_pred = y\n",
    "    index = label.argmax(axis = 1)\n",
    "    y_pred[index,range(y_pred.shape[1])] -= 1.0\n",
    "    dwy = 1 / batch_size * np.dot(y_pred, params['h' + str(time_step)].transpose())\n",
    "    dby = 1 / batch_size * np.sum(y_pred, axis = 1,keepdims = True)\n",
    "    dht = np.dot(Wy.transpose(), y_pred)\n",
    "    dwh = np.zeros_like(Wh)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dwx = np.zeros_like(Wx)\n",
    "    dbx = np.zeros_like(bx)\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    for i in reversed(range(time_step)):\n",
    "        x = X[:,i,:]\n",
    "        dht = (1 - (params['h' + str(i + 1)] ** 2))  * dht ## be careful \n",
    "#         print(dht)\n",
    "        dwh += 1 / batch_size * np.dot(dht, params['h' + str(i)].transpose())\n",
    "        dbh += 1 / batch_size * np.sum(dht, axis = 1, keepdims = True)\n",
    "        dwx += 1 / batch_size * np.dot(dht,x)\n",
    "        dbx += 1 / batch_size * np.sum(dbx, axis = 1, keepdims = True)\n",
    "        dht = np.dot(Wh.transpose(),dht)\n",
    "    Wx -= learning_rate * dwx\n",
    "    bx -= learning_rate * dbx\n",
    "    Wh -= learning_rate * dwh\n",
    "    bh -= learning_rate * dbh\n",
    "    Wy -= learning_rate * dwy\n",
    "    by -= learning_rate * dby\n",
    "    \n",
    "def predict(X):\n",
    "    global Wx,bx,Wh,bh,Wy,by\n",
    "    X = X.reshape(-1,time_step,feature_size)\n",
    "    params['h0'] = np.zeros([hidden_size,batch_size])\n",
    "    for i in range(time_step): ## 每一步拿出所有数据的第一行来进行训练\n",
    "        x = X[:,i,:]\n",
    "        params['h' + str(i + 1)] = np.tanh(np.dot(Wh, params['h'+ str(i)]) + bh + np.dot(Wx,x.transpose()) + bx)\n",
    "    y = softmax(np.dot(Wy,params['h' + str(time_step)]) + by)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterator in range(num_iterator):\n",
    "    train_data,train_label = mn.train.next_batch(batch_size) ### random\n",
    "    forward(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y = mn.test.next_batch(batch_size)\n",
    "y = predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ..., False, False, False])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(y, axis = 0) == np.argmax(test_y,axis = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
