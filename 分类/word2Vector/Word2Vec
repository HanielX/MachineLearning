Word2Vec 是从大量文本语料中以一种无监督的方式学习语义知识的一种模型。
把词的语义特征转换为一种向量的形式，那么在语义相近的词在特征空间中离的越近。
Embedding 其实就是一个映射，将单词从原先所属的空间映射到一个新的空间中。

word2Vec 模型中主要包括两种模型 CBOW 和 Skip-Gram模型
CBOW是给定上下文来预测input-Word，而Skip-Gram是给定input-word来预测上下文
我们并不是读所有的单词都对进行Embedding，而是经过采样的方式来进行的。

我们想要得到是关于Embedding的权重
最终模型的输出是一个概率分布，这个概率表示的是词典中每个词是output_word的可能性

负采样
用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，
负采样每次让一个训练样本仅仅更新一部分的权重，选取一些negative的词的权重进行更新。这样就会
降低在梯度下降的过程中的计算量。

